---
title: "DFA daily progress report"
author: "Mathias"
date: "5/16/2022"
output: 
  html_document:
  toc: true
  toc_float:
    collapsed: false 
    smooth_scroll: false
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(dirname(inputFile), '/field_data_collection_tracker_', format(Sys.Date(), '%Y_%m_%d'),'.html')) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
                      
# read packages
library(tidyverse)
library(lubridate)
library(glue)
library(leaflet)

# read data

dfa_settlement_samples_required <- readxl::read_excel("../inputs/samples_settlement_definition.xlsx") %>% 
  mutate(settlement_name = str_replace(string = settlement_name, pattern = "ii$", replacement = "II"),
         settlement_name = ifelse(settlement_name == "adjumani", str_to_lower(Name_Zn), settlement_name),
         settlement_name = case_when(settlement_name == "alere"~ "alere ii",
                                     settlement_name == "ayilo" ~ "ayiloii",
                                     settlement_name == "mirieyi" ~ "mireyi",
                                     settlement_name == "mungula 2"~ "mungula ii",
                                     settlement_name == "pagirinya"~ "pagrinya",
                                     TRUE ~ settlement_name)) %>% 
  select(settlement_name, smpl_sz)


dfa_host_samples_required <- readxl::read_excel("../inputs/samples_host_sub_definition.xlsx") %>% 
  select(sub_county_name, smpl_sz)
 
dfa_tool_data <- readxl::read_excel("../inputs/UGA2103_Financial_Service_Providers_Assessment_HH_Tool_June2021.xlsx") %>% 
  mutate(uuid = '_uuid',
         start_date = as_date(start),
         start = as_datetime(start),
         end = as_datetime(end),
         latitude = as.numeric(`_geopoint_latitude`),
         longitude = as.numeric(`_geopoint_longitude`)) %>% 
  filter(consent == "yes", respondent_age >=18,
        start_date > as_date("2021-08-29"),
        point_number != "13m.",
        !start_date %in% c(as_date("2021-09-08"), as_date("2021-09-09"),
                           as_date("2021-09-21")),
        !str_detect(string = point_number, pattern = fixed('test', ignore_case = TRUE)),
        !uuid %in% c("27b0ffe2-8d47-4897-b402-1928fd23cfb3",
                      "40d216de-76db-42b7-9105-aea1ce234489",
                      "f2f648df-55d6-4b9d-93ca-aa87c3bc30c7",
                      "4167b891-b1ff-46b1-856b-532dd28e7a1e",
                      "d7bde578-cdc2-4d77-b31b-a15c4cec9d38",
                      "4f3afa4f-a065-4f6d-bd25-9919902f9ce0",
                      "a89d0010-d626-4a4f-8b8c-e83e8fade349",
                      "27ac9f75-3954-4c55-90a3-b5b09984fe7a",
                      "48ee8073-a0d7-460f-9867-304a47bdb1fc",
                      "b0a1c83dcdb24671b9eed78d7a77786f",
                      "c5c9b13aa6cd43338e113d8c647c04ed",
                      "d8936d35-7e1c-48d9-bafa-b25e758c05eb"
          
        ))
          
        
 # days that contain data
dfa_days_for_data_collection <- dfa_tool_data %>% 
  select(start_date) %>% 
  unique() %>% 
  arrange(start_date) %>% 
  pull()
         
         
dfa_data_support_cl_log <- dfa_tool_data %>% 
  select(uuid, status, sub_county_name, settlement_name, latitude, longitude)

# Handling cleaning log
dfa_cl_log <- read_csv(file = "../inputs/combined_logic_spatial_and_others_checks.csv") %>% 
  mutate(adjust_log = ifelse(is.na(adjust_log), "apply_suggested_change", adjust_log)) %>% 
  left_join(dfa_data_support_cl_log, by = "uuid")
  
# change response logs which affect stats during data collection

cl_log_change_response <- dfa_cl_log %>% 
  filter(type == "change response", 
         !is.na(value),
         reviewed == 1,
         adjust_log != "delete_log",
         !issue_id %in%
           c("other_checks", "logic_c_main_language",  "logic_c_internet_awareness")) %>% 
         select(uuid, name, value)
  
  # updated tool data
dfa_updated_tool_data <- dfa_tool_data 
  
# get uuids from cleaning log

  uuids_chg_response <- cl_log_change_response %>% pull(uuid) %>% unique()
  
  for (current_uuid in uuids_chg_response) {
  current_uuid_data <- cl_log_change_response %>% 
    filter(uuid == current_uuid) %>% 
    mutate(value = ifelse(name == "enumerator_id", as.numeric(value), value)) %>% 
    pivot_wider(names_from = "name", values_from = "value", uuid)
  print(current_uuid_data)
  
 # process current updates
  dfa_current_updated <- dfa_updated_tool_data %>% 
    rows_update(y = current_uuid_data, by = "uuid")
  
  # update the parent dataset with current updates
  dfa_updated_tool_data <- dfa_current_updated
} 

  # enumerator performance data
dfa_enum_performance <- dfa_updated_tool_data %>% 
  mutate(int.survey_time_interval = lubridate::time_length(end - start, unit = "min"),
         int.survey_time_interval = ceiling(int.survey_time_interval)) 

```

## Summary of the surveys done

> There are **`r nrow(dfa_updated_tool_data)`** total number of surveys done as of **`r
dfa_days_for_data_collection[length(dfa_days_for_data_collection)]`**.

### Settlements:  **`r dfa_updated_tool_data %>% filter (status == "refugee") %>% nrow() `** surveys

```{r, echo=FALSE}
dfa_refugee_samp_per_settlement <- dfa_settlement_samples_required %>% 
  group_by(settlement_name) %>% 
  summarise(required_samples = sum(smpl_sz, na.rm = TRUE))

dfa_cl_surveys_for_deletion <- dfa_cl_log %>% 
  filter(status == "refugee", type == "remove_survey", reviewed == 1, adjust_log != "delete_log") %>%
  group_by(settlement_name) %>% 
  distinct(uuid) %>%
  summarise(surveys_for_deletion = n())

dfa_updated_tool_data %>% 
  filter(status == "refugee") %>% 
  group_by(district_name, settlement_name) %>% 
  summarise(number_of_surveys = n()) %>% 
  arrange(district_name) %>% 
  left_join(dfa_refugee_samp_per_settlement, by = "settlement_name") %>% 
  left_join(dfa_cl_surveys_for_deletion, by = "settlement_name") %>% 
  mutate(int.surveys_and_deletion = ifelse(is.na(surveys_for_deletion), number_of_surveys, number_of_surveys - surveys_for_deletion),
         remaining_surveys = required_samples - int.surveys_and_deletion ) %>% 
  select(-int.surveys_and_deletion) %>% 
  DT::datatable()
```
### Host community: **`r dfa_updated_tool_data %>% filter(status == "host_community") %>% nrow()`** surveys 

```{r, echo=FALSE} 
dfa_host_samp_per_sub_county <- dfa_host_samples_required %>% 
  group_by(sub_county_name) %>% 
  summarise(required_samples = sum(smpl_sz, na.rm = TRUE))

dfa_cl_surveys_for_deletion <- dfa_cl_log %>% 
  filter(status == "host_community", type == "remove_survey", reviewed == 1, adjust_log != "delete_log") %>%
  group_by(sub_county_name) %>% 
  distinct(uuid) %>%
  summarise(surveys_for_deletion = n())

dfa_updated_tool_data %>% 
  filter(status == "host_community") %>% 
  group_by(district_name, sub_county_name) %>% 
  summarise(number_of_surveys = n()) %>% 
  arrange(district_name) %>% 
  left_join(dfa_host_samp_per_sub_county, by = "sub_county_name") %>% 
  left_join(dfa_cl_surveys_for_deletion, by = "sub_county_name") %>% 
  mutate(int.surveys_and_deletion = ifelse(is.na(surveys_for_deletion), number_of_surveys, number_of_surveys - surveys_for_deletion),
         remaining_surveys = required_samples - int.surveys_and_deletion ) %>% 
  select(-int.surveys_and_deletion) %>% 
  DT::datatable()


```
### Daily enumerator perfomance
The average time for all the data is: **`r round(mean(dfa_enum_performance$int.survey_time_interval), 0)`** Minutes


```{r, echo = FALSE}
dfa_enum_performance %>% 
group_by(district_name, enumerator_id) %>% 
  summarise(number_of_interviews_done = n(), `average_survey_time(minutes)` = round(mean(int.survey_time_interval, na.rm = TRUE), 0))%>%
  DT::datatable()
```
## Looking into the cleaning log

### Number of issues by issue_id

```{r, echo = FALSE}
dfa_cl_log %>% 
  group_by(issue_id) %>% 
  summarise(number_of_issues_by_issue_id = n()) %>%
  DT::datatable()
```
### Number of issues by enumerator

```{r, echo = FALSE}
dfa_cl_log %>% 
  group_by(enumerator_id) %>% 
  summarise(number_of_issues_by_enumerator_id = n()) %>% 
  DT::datatable()
```
### Number of issues by enumerator and issue_id

```{r, echo = FALSE}
dfa_cl_log %>% 
  group_by(enumerator_id, issue_id) %>% 
  summarise(number_of_issues_by_enumerator_and_by_issue_id = n()) %>%
  DT::datatable()
```
### Enumerators with surveys for deletion

```{r, echo = FALSE}
dfa_cl_log %>% 
  filter(type == "remove_survey", reviewed == 1, adjust_log != "delete_log") %>% 
  group_by(enumerator_id) %>% 
  summarise(number_of_surveys_for_deletion_by_enumerator = n()) %>%
  DT::datatable()



```

